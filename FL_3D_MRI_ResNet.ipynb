{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FL_3D_MRI_ResNet",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM5dCZOYhsxVaY6HQtu/ZZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunghwan-moon/2021_capstone_BrainAgePrediction_by_Federated_Learning/blob/main/FL_3D_MRI_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWwyNIZgvJkj"
      },
      "source": [
        "#!pip install tensorflow_federated\n",
        "#!pip install nilearn\n",
        "#!pip install transformations\n",
        "#!pip install nest_asyncio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uOzLyzTvJm2"
      },
      "source": [
        "import collections\n",
        "from nilearn import image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import threading\n",
        "from transformations import rotation_matrix\n",
        "from random import gauss\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "import operator\n",
        "from scipy.ndimage.interpolation import shift,rotate\n",
        "import pandas as pd\n",
        "import os\n",
        "import scipy.ndimage as nd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score,mean_absolute_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "import glob\n",
        "import nibabel as nib\n",
        "from collections import defaultdict\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "#from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, add, concatenate,GlobalAveragePooling3D,Conv3D,LeakyReLU,ELU, MaxPooling3D,AveragePooling3D\n",
        "from tensorflow.keras.regularizers import l2 as L2\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from tensorflow.keras.optimizers import Adam, SGD,Adagrad\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Input, InputLayer, InputSpec\n",
        "#from tensorflow.keras import backend as K\n",
        "#from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "#import tensorflow.compat.v1 as tf\n",
        "# tf.compat.v1.enable_v2_behavior()\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "from tensorflow.compat.v1 import keras\n",
        "from tensorflow.compat.v1.keras.layers import Dense, Dropout, Activation, Flatten, Input, add, concatenate,GlobalAveragePooling3D,Conv3D,LeakyReLU,ELU, MaxPooling3D,AveragePooling3D"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSI_JnviudUC",
        "outputId": "4e729ce7-ded5-4df6-8fe3-5aa7dd441e1c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y_mqvdkvSuv"
      },
      "source": [
        "def getIXIData():\n",
        "  data = pd.read_csv('/content/drive/MyDrive/소융_캡스톤/뇌 MRI 사진_교수님/raw/IXI.csv')\n",
        "  data.drop(['ETHNIC_ID','MARITAL_ID','OCCUPATION_ID','QUALIFICATION_ID','DOB','DATE_AVAILABLE','STUDY_DATE'], axis =1, inplace = True)\n",
        "  data[\"IXI_ID\"] = data[\"IXI_ID\"].astype(str)\n",
        "  data = data.drop_duplicates(['IXI_ID'])\n",
        "  data['feature'] = np.NaN\n",
        "  data = data.set_index('IXI_ID')\n",
        "\n",
        "  #RAM 초과로 인해 250개 dataset만 가지고 진행 -> 실제 데이터는 mri 폴더에 존재\n",
        "  path = '/content/drive/MyDrive/소융_캡스톤/뇌 MRI 사진_교수님/dataset 일부/' \n",
        "  paths = glob.glob(path +'wm*.nii')\n",
        "\n",
        "  id_with_data = {}\n",
        "  for img_path in paths:\n",
        "    img_index = img_path.split('/')[7][5:8]\n",
        "    if(img_index[0] == '0' and img_index[1] == '0'):\n",
        "      img_index = img_index[2]\n",
        "    if(img_index[0] == '0' and img_index[1] != '0'):\n",
        "      img_index = img_index[1:3]\n",
        "    id_with_data[img_index] = image.get_data(img_path)\n",
        "\n",
        "  feature_matrix = []\n",
        "  drop_matrix = []\n",
        "  copy_data = data.copy()\n",
        "\n",
        "  for i in range(len(data)): \n",
        "    for key, value in id_with_data.items():\n",
        "      found = False\n",
        "      if(data.index[i] == key):\n",
        "        found = True\n",
        "        feature_matrix.append(value)\n",
        "        del id_with_data[key]\n",
        "        break\n",
        "    if(found == False):\n",
        "      drop_matrix.append(data.index[i])\n",
        "\n",
        "  for i in range(len(drop_matrix)):\n",
        "    copy_data.drop(drop_matrix[i], inplace = True)\n",
        "\n",
        "  copy_data[\"feature\"] = feature_matrix\n",
        "  copy_data.dropna(axis=0, inplace = True)\n",
        "  copy_data.reset_index(drop=True, inplace = True)\n",
        "\n",
        "  train, val = train_test_split(copy_data, test_size = 0.2, random_state =100)\n",
        "\n",
        "  return train,val"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY9gU3pdyLoQ"
      },
      "source": [
        "def df_to_array(dataframe):\n",
        "  temp_lst = []\n",
        "\n",
        "  for i in range(len(dataframe)):\n",
        "    feature = dataframe[\"feature\"][i]\n",
        "    temp_lst.append(feature)\n",
        "\n",
        "  result = np.array(temp_lst)\n",
        "\n",
        "  return result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdijWo5evsk2"
      },
      "source": [
        "train ,val = getIXIData()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn_s4PtavweE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b36bd2b-54f6-4aed-a28a-7c2afa0eb3b6"
      },
      "source": [
        "train[\"feature\"].reset_index(inplace= True, drop = True)\n",
        "val[\"feature\"].reset_index(inplace= True, drop = True)\n",
        "x_train = df_to_array(train).astype(np.float32)\n",
        "y_train = train[\"AGE\"].astype(np.float16)\n",
        "\n",
        "x_test = df_to_array(val).astype(np.float32)\n",
        "y_test = np.array(val[\"AGE\"].astype(np.float16))\n",
        "\n",
        "num_train_image = len(x_train)\n",
        "num_test_image= len(x_test)\n",
        "print(\"Train set: \", num_train_image)\n",
        "print(\"Test set: \", num_test_image)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set:  193\n",
            "Test set:  49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX_FEmmWyFmX"
      },
      "source": [
        "#client Data Non-iid 형태로 만들기\n",
        "client_train_dataset = collections.OrderedDict()\n",
        "client_num = 10\n",
        "for i in range(0, client_num): # Client 10명으로 제한\n",
        "  client_key = \"client_\" + str(i+1) # client_1, client_2 .....\n",
        "\n",
        "  # non-iid를 위한 random sampling\n",
        "  front_train = random.randint(1, num_train_image) \n",
        "  rear_train = random.randint(front_train, num_train_image)\n",
        "\n",
        "  front_test = random.randint(1, num_test_image)\n",
        "  rear_test = random.randint(front_test, num_test_image)\n",
        "\n",
        "  train_data = collections.OrderedDict((('pixels', x_train[front_train:rear_train]),('label', y_train[front_train:rear_train])))\n",
        "  test_data =  collections.OrderedDict((('pixels', x_test[front_test:rear_test]),('label', y_test[front_test:rear_test])))\n",
        "\n",
        "  client_train_dataset[client_key] = train_data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX1pA_xSJiR6"
      },
      "source": [
        "#tensorSliceClientData를 simulation의 instance로 변환\n",
        "train_dataset = tff.simulation.datasets.TestClientData(client_train_dataset)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEzdKf7RJiUP"
      },
      "source": [
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 3\n",
        "SHUFFLE_BUFFER = 1\n",
        "PREFETCH_BUFFER = 1\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.reshape(element['pixels'], [-1, 121, 145, 121]),\n",
        "        y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
        "      BATCH_SIZE).prefetch(PREFETCH_BUFFER).map(batch_format_fn)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvBS3OTQDDws"
      },
      "source": [
        "example_dataset = train_dataset.create_tf_dataset_for_client(train_dataset.client_ids[0])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRNtiwME4ygP"
      },
      "source": [
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "  ]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwLkZsUYJiWY"
      },
      "source": [
        "def make_federated_data(client_data, client_ids):\n",
        "  result_list = []\n",
        "  for id in client_ids:\n",
        "    preprocess_data = preprocess(client_data.create_tf_dataset_for_client(id))\n",
        "    sample = tf.nest.map_structure(lambda x: x.numpy(),next(iter(preprocess_data)))\n",
        "    print(\"client \",id, \" size : \", len(sample[\"y\"]))\n",
        "    result_list.append(preprocess_data)\n",
        "  return result_list"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuY_ubCZ5SsU",
        "outputId": "0b54d9ac-3741-498e-fa5f-7163ff948e81"
      },
      "source": [
        "sample_clients = train_dataset.client_ids[0:3] #client 중에 몇명을 선발하여 Train을 할 것인가?\n",
        "\n",
        "federated_train_data = make_federated_data(train_dataset, sample_clients)\n",
        "\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))\n",
        "print(\"input_spec : {n}\".format(n = federated_train_data[0].element_spec))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "client  client_1  size :  3\n",
            "client  client_2  size :  3\n",
            "client  client_3  size :  3\n",
            "Number of client datasets: 3\n",
            "First dataset: <MapDataset shapes: OrderedDict([(x, (None, 121, 145, 121)), (y, (None, 1))]), types: OrderedDict([(x, tf.float32), (y, tf.float16)])>\n",
            "input_spec : OrderedDict([('x', TensorSpec(shape=(None, 121, 145, 121), dtype=tf.float32, name=None)), ('y', TensorSpec(shape=(None, 1), dtype=tf.float16, name=None))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdBBV4BvJiYK"
      },
      "source": [
        "def generateAgePredictionResNet(inputShape,paddingType = 'same',initType='he_uniform',regAmount=0.00005,dropRate=0.2):\n",
        "    t1Input = Input(inputShape+(1,))\n",
        "    \n",
        "    with tf.name_scope('ResBlock0'):\n",
        "        inputs = t1Input\n",
        "        features = 8\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(inputs)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        hidden = ELU(alpha=1.0)(hidden)\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(hidden)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        shortcut = Conv3D(features, (1,1,1), strides=(1,1,1), padding=paddingType,kernel_initializer=initType)(inputs)\n",
        "        hidden = add([shortcut,hidden])\n",
        "        outputs = ELU(alpha=1.0)(hidden)\n",
        "        \n",
        "    pooling = MaxPooling3D(pool_size=(2,2,2),strides=(2,2,2), padding=paddingType)(outputs)\n",
        "    \n",
        "    with tf.name_scope('ResBlock1'):\n",
        "        inputs = pooling\n",
        "        features = 16\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(inputs)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        hidden = ELU(alpha=1.0)(hidden)\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(hidden)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        shortcut = Conv3D(features, (1,1,1), strides=(1,1,1), padding=paddingType,kernel_initializer=initType)(inputs)\n",
        "        hidden = add([shortcut,hidden])\n",
        "        outputs = ELU(alpha=1.0)(hidden)\n",
        "        \n",
        "    pooling = MaxPooling3D(pool_size=(2,2,2),strides=(2,2,2), padding=paddingType)(outputs)\n",
        "\n",
        "    with tf.name_scope('ResBlock2'):\n",
        "        inputs = pooling\n",
        "        features = 32\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(inputs)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        hidden = ELU(alpha=1.0)(hidden)\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(hidden)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        shortcut = Conv3D(features, (1,1,1), strides=(1,1,1), padding=paddingType,kernel_initializer=initType)(inputs)\n",
        "        hidden = add([shortcut,hidden])\n",
        "        outputs = ELU(alpha=1.0)(hidden)\n",
        "        \n",
        "    pooling = MaxPooling3D(pool_size=(2,2,2),strides=(2,2,2), padding=paddingType)(outputs)\n",
        " \n",
        "    \n",
        "    with tf.name_scope('ResBlock3'):\n",
        "        inputs = pooling\n",
        "        features = 64\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(inputs)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        hidden = ELU(alpha=1.0)(hidden)\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(hidden)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        shortcut = Conv3D(features, (1,1,1), strides=(1,1,1), padding=paddingType,kernel_initializer=initType)(inputs)\n",
        "        hidden = add([shortcut,hidden])\n",
        "        outputs = ELU(alpha=1.0)(hidden)\n",
        "        \n",
        "        \n",
        "    pooling = MaxPooling3D(pool_size=(2,2,2),strides=(2,2,2), padding=paddingType)(outputs)\n",
        "  \n",
        "    \n",
        "    with tf.name_scope('ResBlock4'):\n",
        "        inputs = pooling\n",
        "        features = 128\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(inputs)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        hidden = ELU(alpha=1.0)(hidden)\n",
        "        hidden = Conv3D(features, (3, 3, 3), padding=paddingType,kernel_regularizer=L2(regAmount),kernel_initializer=initType)(hidden)\n",
        "        hidden = BatchNormalization(renorm=True)(hidden)\n",
        "        shortcut = Conv3D(features, (1,1,1), strides=(1,1,1), padding=paddingType,kernel_initializer=initType)(inputs)\n",
        "        hidden = add([shortcut,hidden])\n",
        "        outputs= ELU(alpha=1.0)(hidden)\n",
        "        \n",
        "    pooling = MaxPooling3D(pool_size=(2,2,2),strides=(2,2,2), padding=paddingType)(outputs)\n",
        "\n",
        "    hidden = Flatten()(pooling)\n",
        "    \n",
        "    hidden = Dense(128,kernel_regularizer=L2(regAmount),kernel_initializer=initType,name='FullyConnectedLayer')(hidden)\n",
        "    hidden = ELU(alpha=1.0)(hidden)\n",
        "    hidden = Dropout(dropRate)(hidden)\n",
        "       \n",
        "    prediction = Dense(1,kernel_regularizer=L2(regAmount), name='AgePrediction')(hidden)\n",
        "    model = Model(inputs=[t1Input],outputs=prediction)\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqNBXSI1J1ul"
      },
      "source": [
        "def model_fn():\n",
        "    keras_model = generateAgePredictionResNet((121,145,121),paddingType = 'same',initType='he_uniform',regAmount=0.00005,dropRate=0.2)\n",
        "    return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=federated_train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "      metrics=[tf.keras.metrics.MeanAbsoluteError()])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc2XHN_nJ1xF"
      },
      "source": [
        "def initialize_fn():\n",
        "  model = model_fn()\n",
        "  return model.trainable_variables"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TA7GiihL8w2"
      },
      "source": [
        "def next_fn(server_weights, federated_dataset):\n",
        "  server_weights_at_client = broadcast(server_weights)\n",
        "  client_weights = client_update(federated_dataset, server_weights_at_client)\n",
        "  mean_client_weights = mean(client_weights)\n",
        "  server_weights = server_update(mean_client_weights)\n",
        "  \n",
        "  return server_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U62TEoSGJ1zg"
      },
      "source": [
        "@tf.function\n",
        "def client_update(model, dataset, server_weights, client_optimizer):\n",
        "  print(\"client_update tf.fn\")\n",
        "  client_weights = model.trainable_variables\n",
        "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
        "                        client_weights, server_weights)\n",
        "  \n",
        "  for batch in dataset:\n",
        "      with tf.GradientTape() as tape:\n",
        "        outputs = model.forward_pass(batch)\n",
        "\n",
        "      grads = tape.gradient(outputs.loss, client_weights)\n",
        "      grads_and_vars = zip(grads, client_weights)\n",
        "\n",
        "      client_optimizer.apply_gradients(grads_and_vars)\n",
        "\n",
        "  return client_weights"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW43-L6XMsp_"
      },
      "source": [
        "@tf.function\n",
        "def server_update(model, mean_client_weights):\n",
        "  model_weights = model.trainable_variables\n",
        "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
        "                        model_weights, mean_client_weights)\n",
        "  return model_weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R9AlzSoM68n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c143295a-5f14-4dcf-e41d-d03cb6cc120d"
      },
      "source": [
        "@tff.tf_computation\n",
        "def server_init():\n",
        "  model = model_fn()\n",
        "  return model.trainable_variables"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization.py:524: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRTAvFYSNLWw"
      },
      "source": [
        "@tff.federated_computation\n",
        "def initialize_fn():\n",
        "  return tff.federated_value(server_init(),tff.SERVER)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKU83IOARr4Q"
      },
      "source": [
        "tf_dataset_type = tff.SequenceType(model_fn().input_spec)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FXXONLj2tJD7",
        "outputId": "85691d02-eda8-4155-ec41-773d6b509ce8"
      },
      "source": [
        "str(tf_dataset_type)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<x=float32[?,121,145,121],y=float16[?,1]>*'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayeRWd3iPv-1"
      },
      "source": [
        "model_weights_type = server_init.type_signature.result"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fodKTfXYjgQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c5f658-9750-4790-94b0-a839d1e1a28d"
      },
      "source": [
        "@tff.tf_computation(tf_dataset_type, model_weights_type)\n",
        "def client_update_fn(tf_dataset, server_weights):\n",
        "  model = model_fn()\n",
        "  client_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01) #Adam, SGD\n",
        "  return client_update(model, tf_dataset, server_weights, client_optimizer)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "client_update tf.fn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjH5-w__P-f3"
      },
      "source": [
        "@tff.tf_computation(model_weights_type)\n",
        "def server_update_fn(mean_client_weights):\n",
        "  model = model_fn()\n",
        "  return server_update(model, mean_client_weights)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt9z2szQByy"
      },
      "source": [
        "federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
        "federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2GLH0pKutrQ-",
        "outputId": "499d2209-1a86-4960-a7de-0e6a0815204d"
      },
      "source": [
        "str(federated_dataset_type)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{<x=float32[?,121,145,121],y=float16[?,1]>*}@CLIENTS'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf81urj0ttyR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "644fbf6f-0f43-432f-90a9-182c7674f606"
      },
      "source": [
        "str(federated_server_type)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<float32[3,3,3,1,8],float32[8],float32[8],float32[8],float32[3,3,3,8,8],float32[8],float32[1,1,1,1,8],float32[8],float32[8],float32[8],float32[3,3,3,8,16],float32[16],float32[16],float32[16],float32[3,3,3,16,16],float32[16],float32[1,1,1,8,16],float32[16],float32[16],float32[16],float32[3,3,3,16,32],float32[32],float32[32],float32[32],float32[3,3,3,32,32],float32[32],float32[1,1,1,16,32],float32[32],float32[32],float32[32],float32[3,3,3,32,64],float32[64],float32[64],float32[64],float32[3,3,3,64,64],float32[64],float32[1,1,1,32,64],float32[64],float32[64],float32[64],float32[3,3,3,64,128],float32[128],float32[128],float32[128],float32[3,3,3,128,128],float32[128],float32[1,1,1,64,128],float32[128],float32[128],float32[128],float32[10240,128],float32[128],float32[128,1],float32[1]>@SERVER'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LusuOlaQDUu"
      },
      "source": [
        "@tff.federated_computation(federated_server_type, federated_dataset_type)\n",
        "def next_fn(server_weights, federated_dataset):\n",
        "  server_weights_at_client = tff.federated_broadcast(server_weights)\n",
        "  client_weights = tff.federated_map(\n",
        "        client_update_fn, (federated_dataset, server_weights_at_client))\n",
        "  mean_client_weights = tff.federated_mean(client_weights)\n",
        "  server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
        "\n",
        "  return server_weights"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP8sJ7iTQHhq"
      },
      "source": [
        "federated_algorithm = tff.templates.IterativeProcess(\n",
        "      initialize_fn=initialize_fn,\n",
        "      next_fn=next_fn\n",
        "    )"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaTdm9mRQSEt"
      },
      "source": [
        "def evaluate(server_state):\n",
        "  keras_model = generateAgePredictionResNet((121,145,121),paddingType = 'same',initType='he_uniform',regAmount=0.00005,dropRate=0.2)\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "      metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
        "  )\n",
        "  results = keras_model.evaluate(x=x_test, y=y_test, batch_size=5)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJLr7i0pQYMd"
      },
      "source": [
        "server_state = federated_algorithm.initialize()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTOKj6MGzuZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5325e91f-8cfb-4067-dca8-c8594299e627"
      },
      "source": [
        "evaluate(server_state)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 17s 162ms/step - loss: 56.6468 - mean_absolute_error: 56.5843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2GFFLyxdOpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b36039-f669-4953-f84d-a663e9385d94"
      },
      "source": [
        "for round in range(100):\n",
        "  print(\"round \",round+1)\n",
        "  server_state = federated_algorithm.next(server_state, federated_train_data)\n",
        "  print(\"\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:60: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:60: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "round  2\n",
            "\n",
            "round  3\n",
            "\n",
            "round  4\n",
            "\n",
            "round  5\n",
            "\n",
            "round  6\n",
            "\n",
            "round  7\n",
            "\n",
            "round  8\n",
            "\n",
            "round  9\n",
            "\n",
            "round  10\n",
            "\n",
            "round  11\n",
            "\n",
            "round  12\n",
            "\n",
            "round  13\n",
            "\n",
            "round  14\n",
            "\n",
            "round  15\n",
            "\n",
            "round  16\n",
            "\n",
            "round  17\n",
            "\n",
            "round  18\n",
            "\n",
            "round  19\n",
            "\n",
            "round  20\n",
            "\n",
            "round  21\n",
            "\n",
            "round  22\n",
            "\n",
            "round  23\n",
            "\n",
            "round  24\n",
            "\n",
            "round  25\n",
            "\n",
            "round  26\n",
            "\n",
            "round  27\n",
            "\n",
            "round  28\n",
            "\n",
            "round  29\n",
            "\n",
            "round  30\n",
            "\n",
            "round  31\n",
            "\n",
            "round  32\n",
            "\n",
            "round  33\n",
            "\n",
            "round  34\n",
            "\n",
            "round  35\n",
            "\n",
            "round  36\n",
            "\n",
            "round  37\n",
            "\n",
            "round  38\n",
            "\n",
            "round  39\n",
            "\n",
            "round  40\n",
            "\n",
            "round  41\n",
            "\n",
            "round  42\n",
            "\n",
            "round  43\n",
            "\n",
            "round  44\n",
            "\n",
            "round  45\n",
            "\n",
            "round  46\n",
            "\n",
            "round  47\n",
            "\n",
            "round  48\n",
            "\n",
            "round  49\n",
            "\n",
            "round  50\n",
            "\n",
            "round  51\n",
            "\n",
            "round  52\n",
            "\n",
            "round  53\n",
            "\n",
            "round  54\n",
            "\n",
            "round  55\n",
            "\n",
            "round  56\n",
            "\n",
            "round  57\n",
            "\n",
            "round  58\n",
            "\n",
            "round  59\n",
            "\n",
            "round  60\n",
            "\n",
            "round  61\n",
            "\n",
            "round  62\n",
            "\n",
            "round  63\n",
            "\n",
            "round  64\n",
            "\n",
            "round  65\n",
            "\n",
            "round  66\n",
            "\n",
            "round  67\n",
            "\n",
            "round  68\n",
            "\n",
            "round  69\n",
            "\n",
            "round  70\n",
            "\n",
            "round  71\n",
            "\n",
            "round  72\n",
            "\n",
            "round  73\n",
            "\n",
            "round  74\n",
            "\n",
            "round  75\n",
            "\n",
            "round  76\n",
            "\n",
            "round  77\n",
            "\n",
            "round  78\n",
            "\n",
            "round  79\n",
            "\n",
            "round  80\n",
            "\n",
            "round  81\n",
            "\n",
            "round  82\n",
            "\n",
            "round  83\n",
            "\n",
            "round  84\n",
            "\n",
            "round  85\n",
            "\n",
            "round  86\n",
            "\n",
            "round  87\n",
            "\n",
            "round  88\n",
            "\n",
            "round  89\n",
            "\n",
            "round  90\n",
            "\n",
            "round  91\n",
            "\n",
            "round  92\n",
            "\n",
            "round  93\n",
            "\n",
            "round  94\n",
            "\n",
            "round  95\n",
            "\n",
            "round  96\n",
            "\n",
            "round  97\n",
            "\n",
            "round  98\n",
            "\n",
            "round  99\n",
            "\n",
            "round  100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJyDzJ6manm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e075b12-3396-4909-87f1-01d1fbe795fc"
      },
      "source": [
        "evaluate(server_state)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 118ms/step - loss: 66.9455 - mean_absolute_error: 66.8830\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh9-WoWlQwjE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}